#%Module
# singularity

proc ModulesHelp { } {
   puts stderr "Provide a user environment suitable for running Singularity containers"
}
module-whatis "Provide a user environment suitable for running Singularity containers"

set appname singularity
set machine titan
set version 2.2.1

# Ensure that the correct environment variables are picked up in the container
# assuming TitanPrep.sh has been run on the image

if { [ is-loaded PrgEnv-pgi ] } {
  module swap PrgEnv-pgi PrgEnv-gnu
}
if { [ is-loaded PrgEnv-intel ] } {
  module swap PrgEnv-intel PrgEnv-gnu
}
if { [ is-loaded PrgEnv-cray ] } {
  module swap PrgEnv-cray PrgEnv-gnu
}
module swap gcc gcc/4.9.3

setenv SYSUTILS_DEFAULT_DIR [exec readlink -f /opt/cray/sysutils/default]
setenv WLM_DEFAULT_DIR [exec readlink -f /opt/cray/wlm_detect/default]
set CRAY_NVIDIA_DRIVER_LIB_DIR [exec readlink -f /opt/cray/nvidia/default/lib64]
set CRAY_NVIDIA_DRIVER_BIN_DIR [exec readlink -f /opt/cray/nvidia/default/bin]

# Unset a few variables within the container
setenv SINGULARITYENV_PYTHONSTARTUP ""
setenv SINGULARITYENV_PKG_CONFIG_PATH ""

# LD_PRELOAD Cray's MPI libraries
# These should be the abi variants but currently the abi libraries point to the standard libmpich libraries
set C_MPICH_SO [exec readlink -f /opt/cray/mpt/default/gni/mpich-gnu/4.9/lib/libmpich.so]
set CXX_MPICH_SO [exec readlink -f /opt/cray/mpt/default/gni/mpich-gnu/4.9/lib/libmpichcxx.so]
set F_MPICH_SO [exec readlink -f /opt/cray/mpt/default/gni/mpich-gnu/4.9/lib/libfmpich.so]
setenv SINGULARITYENV_LD_PRELOAD ${C_MPICH_SO}:${CXX_MPICH_SO}:${F_MPICH_SO}

# Ensure Cray specific libraries are appended to LD_LIBRARY_PATH
# @TODO: Remove CRAY_NVIDIA components when singularity/2.3 released
set CRAY_LD_LIBRARY_PATH $::env(CRAY_LD_LIBRARY_PATH)

# These are needed to keep Cray PMI happy when LD_PRELOADING MPI libraries
setenv SINGULARITYENV_PMI_NO_FORK 1
setenv SINGULARITYENV_PMI_NO_PREINITIALIZE 1

setenv SINGULARITYENV_MODULE_LOADED 1
#
##-- end
#

